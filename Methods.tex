\chapter{Tools \& Methods}
\label{ch:methods}

\epigraph{\textit{The infinite is obvious and everywhere. To engage the finite takes courage.}}{Hunter Hunt-Hendrix -- \textit{Transcendental Black Metal}}

\vspace{1cm}

\par\noindent In this Chapter, I describe the tools and methods I employed as part of my studies.  In Section \ref{sec:sat} I describe the scientific instruments which were used to take the data I present in this thesis.  In Section \ref{sec:tec} I describe a number of methods and algorithms created by others which I make use of in my analysis.  I also present algorithms I have created as part of my studies.

\section{Instrumentation}

\label{sec:sat}

\par The atmosphere of the Earth is opaque to X-rays and gamma-rays, so we must use space-based observatories in order to study high-energy astrophysical phenomena.  A number of satellites dedicated to the study of X-rays have been launched over the years, starting with \textit{Uhuru}\index{Uhuru@\textit{Uhuru}} in 1970 \citep{Giacconi_Uhuru} and culminating, most recently, with NASA-operated \textit{NICER}\index{NICER@\textit{NICER}} \citep{Gendreau_Nicer} and the Chinese-operated \textit{Insight}\index{Insight@\textit{Insight}} \citep{Li_HXMT} in 2017.  I use data from a number of these missions in the research reported in this thesis; in particular I use data from the NASA satellites \textit{RXTE}, \textit{Swift}, \textit{Chandra} and \textit{NuSTAR}, the European satellites \textit{XMM-Newton} and \textit{INTEGRAL}, and the Japanese satellite \textit{Suzaku}\indexrxte\indexswift\indexchandra\indexnustar\indexxmm\indexintegral\indexsuzaku.  This section introduces the instruments used in my studies, as well as the tools used to extract their data for further analysis.
\index{Rossi X-Ray Timing Experiment@the \textit{Rossi X-Ray Timing Experiment}|see {\textit{RXTE}}}
\index{Neil Gehrels Swift Observatory@the \textit{Neil Gehrels Swift Observatory}|see {\textit{Swift}}}
\index{X-Ray Multi-Mirror Mission@the \textit{X-Ray Multi-Mirror Mission}|see {\textit{XMM-Newton}}}
\index{Nuclear Spectroscopic Telescope Array@the \textit{Nuclear Spectroscopic Telescope Array}|see {\textit{NuSTAR}}}
\index{International Gamma-Ray Astrophysics Laboratory@the \textit{International Gamma-Ray Astrophysics Laboratory}|see {\textit{INTEGRAL}}}
\index{Hubble Space Telescope@the \textit{Hubble Space Telescope}|see {\textit{HST}}}
\index{Neutron Star Interior Composition Explorer@the \textit{Neutron Star Interior Composition Explorer}|see {\textit{NICER}}}
\index{Advanced Telescope for High Energy Astrophysics@the \textit{Advanced Telescope for High Energy Astrophysics}|see {\textit{ATHENA}}}
\index{Compton Gamma Ray Observatory@the \textit{Compton Gamma Ray Observatory}|see {\textit{CGRO}}}
\index{Australian Telescope Compact Array@the Australian Telescope Compact Array|see {ATCA}}

\subsection{The \textit{Rossi X-Ray Timing Experiment}}

\par The \textit{Rossi X-Ray Timing Experiment}\indexrxte, more commonly known as \textit{RXTE}, was a NASA-operated satellite launched from Cape Canaveral in the United States on December 30, 1995 \citep{Bradt_RXTE}.  \textit{RXTE} was primarily an X-ray observatory, constructed specifically to study X-ray variability seen in X-ray Binaries \citep{Bradt_XTEaims}.  The observatory operated until January 5, 2012, when it was decommissioned.  \textit{RXTE} likely re-entered Earth's atmosphere over Venezuela on April 30, 2018.
\par \textit{RXTE} carried three scientific instruments.  The main instruments were a pair of X-ray telescopes: the Proportional Counter Array (PCA\indexpca, \citealp{Jahoda_PCA}) and the High Energy X-Ray Timing Experiment (HEXTE\indexhexte, \citealp{Gruber_HEXTE}).  The satellite also carried an X-ray All-Sky Monitor (ASM\indexasm, \citealp{Levine_ASM}).  PCA consisted of 5 Proportional Counting Units (PCUs) which were sensitive between $\sim2$--$60$\,keV.  The instrument had an excellent time resolution approaching 1\,$\mu$s, and an energy resolution of $\sim18\%$ at 6\,keV.  X-rays were guided onto the detectors by a collimator, resulting in an instrumental field of view with a full-width half-maximum of 1$^\circ$.  PCA had a 6500\,cm$^2$ collecting area, and no angular resolution \citep{Jahoda_PCA}.
\par The HEXTE\indexhexte\ instrument \citep{Gruber_HEXTE} provided complimentary coverage at higher energies, being sensitive in the $\sim15$--$250$\,keV range.  This instrument consisted of 8 detectors on two separate arms, with a total collecting area of 1600\,cm$^2$, and had a similar field of view to that of PCA.  The time resolution was 8\,$\mu$s, and the energy resolution was 15\% at 60\,keV \citep{Gruber_HEXTE}.
\par Finally, ASM\indexasm\ was a medium-energy X-ray all sky-monitor which covered 80\% of the sky every 90 minutes.  It was sensitive in the range 2--10\,keV, with a total collecting area of 90\,cm$^2$ and a spatial resolution of $3'\times15'$ \citep{Levine_ASM}.  Due to its near continual coverage of the sky, ASM was excellent for long-term monitoring of transients in the soft X-ray sky.

\subsubsection{Data Formatting}

\par Much of the work in this thesis is based largely on data from \textit{RXTE}/PCA, which is freely available through the HEASARC\index{HEASARC} archive maintained by NASA's Goddard Space Flight Centre\footnote{\url{https://heasarc.gsfc.nasa.gov/cgi-bin/W3Browse/w3browse.pl}}.  PCA, as well as many other X-ray instruments, records data in one of two forms:
\begin{itemize}
\item \textbf{Event-Mode Data\index{Event-mode data}:} A list of photon arrival times.  Depending on the instrument and observing mode, each of these times will have an associated channel\index{Channel}, information about where in the detector the photon hit and a flag indicating the pattern that the photon made on the detector.
\item \textbf{Binned Data\index{Binned data}:} A list of evenly spaced time bins with the number of photons which arrived during each.  Depending on the instrument and observing mode, this may be accompanied by some information on the channel distribution of photons arriving in each bin.
\end{itemize}
\par Both event-mode and binned-mode data are stored in a Flexible Image Transit System\index{Flexible Image Transit System|see {\texttt{FITS}}}\index{FITS@\texttt{FITS}} (\texttt{.fits}) format.  This is a hierarchical data format consisting of a number of `Header Data Units' (HDUs), each of which contains data in some format and a header with details of the format.  In addition to either an event list or a table of binned data, astronomical FITS files also contain a list of Good Time Intervals (GTIs) during which the satellite was functioning normally, as well as an amount of housekeeping information such as the start and end times of the observation.
\par The channel\index{Channel} a photon falls into is determined by its energy, although the channel-to-energy conversion for a particular instrument changes over time as the instrument degrades or settings are altered.  The approximate channel-to-energy conversions for PCA can be found at \url{https://heasarc.gsfc.nasa.gov/docs/xte/e-c_table.html}.
\par For PCA observations of faint objects, event mode data with full energy information (referred to as \texttt{goodxenon}-mode\indexgx\ data) is generally available.  However when brighter objects were observed, telemetry constraints sometimes prevented this full information from being transmitted to Earth.  In all observations, a number of alternative data products are available; \texttt{Standard1}\indexsto\ data (binned data with 0.125\,s time resolution but no energy information), \texttt{Standard2}\indexstt\ data (binned data with 16\,s time resolution, divided into 129 bins by channel) and a number of other data products with various time and energy resolutions.  \texttt{Standard2} data are useful for studying spectral variability over long timescales, while \texttt{Standard1} data are useful for studying fluctuations in X-ray luminosity over shorter timescales.  I use \texttt{goodxenon} data when available, as this allowed me to use the maximum possible time and energy resolutions.  When \texttt{goodxenon} was not I available I used various other datamodes, including \texttt{Standard1} and a number of different event-mode datamodes.

\subsubsection{Data Extraction \& Background Correction}

\par To perform science with PCA or other instruments, one must extract science products (such as lightcurves\index{Lightcurve}, power spectra\index{Fourier analysis}\index{Power spectrum|see {Fourier analysis}} and energy spectra\index{Spectroscopy}\index{Energy spectrum|see {Spectroscopy}}) from the raw data.  Tools to create lightcurves and power spectra from PCA\indexpca\ data are available as part of \texttt{FTOOLS}\index{FTOOLS@\texttt{FTOOLS}} \footnote{\url{https://heasarc.gsfc.nasa.gov/ftools/}}, a free NASA-maintained suite of software for manipulating \texttt{.fits}\index{FITS@\texttt{FITS}} formatted data.  These scripts make use of CALDBs: freely available databases of calibration files provided by NASA for a number of active and historical X-ray telescopes (e.g. \citealp{Graessle_ChaCALDB}).  I also wrote my own software \texttt{PANTHEON}\index{PANTHEON@\texttt{PANTHEON}} (Python ANalytical Tools for High-energy Event-data manipulatiON, presented in Appendix \ref{app:PAN}) to extract a number of additional products, such as power spectra and spectrograms.
\par In astronomy, the general way to subtract background\index{Background subtraction} from data is by selecting an empty piece of sky from the same observation as the source of interest, and then subtract the counts in one from the other.  However as PCA had no imaging capability, this is not possible with data from this instrument.  Instead, the \textit{RXTE} Guest Observatory Facility provides background models, which estimate the background of an observation based on the known X-ray background near the pointing direction and how the radiation environment of the spacecraft changes over its orbit.  Two background models are available, for faint\footnote{\url{http://heasarc.gsfc.nasa.gov/FTP/xte/calib_data/pca_bkgd/Faint/pca_bkgd_cmfaintl7_eMv20051128.mdl}} ($<40$\,cts\,s$^{-1}$\,PCU$^{-1}$) and bright\footnote{\url{http://heasarc.gsfc.nasa.gov/FTP/xte/calib_data/pca_bkgd/Sky_VLE/pca_bkgd_cmbrightvle_eMv20051128.mdl}} ($>40$\,cts\,s$^{-1}$\,PCU$^{-1}$) sources; these can be used in conjunction with the \texttt{pcabackest} tool in \texttt{FTOOLS}\index{FTOOLS@\texttt{FTOOLS}} to estimate the background as a function of time and energy.  This spectral model can then be subtracted from binned PCA data.
\par As the PCA background models do not subtract the contributions from other sources in the field of view, I also use a different technique to subtract background from observations of GRO J1744-28\index{Bursting Pulsar} (which is in a very crowded region of the sky near the Galactic centre).  To try and account for these other sources, I instead chose an observation of the region of GRO J1744-28 taken while this source was in quiescence\index{Quiescence}; I assume that all photons in this observation must be from the particle background, the cosmic background or another source in the field of view.  Although this method does subtract some of the background contributed from other sources in the field, it must be treated with caution as these other sources are likely also variable.
\par To compare photometry data from PCA with data from other instruments, I normalise the data by the flux from the Crab nebula\index{Crab nebula}.  The Crab is a commonly used reference source in astronomy due to its apparent brightness and low variability across a wide portion of the electromagnetic spectrum.  To Crab-normalise PCA data from a given observation, I take the PCA observation of the Crab which is closest in time to the observation of interest and in the same gain epoch.  This follows the method employed in \citet{Altamirano_CrabNorm}.
\par In addition to PCA, I also make use of data from \textit{RXTE}/ASM.  Long-term lightcurves from ASM are available on the ASM Light Curves Overview web page (\url{http://xte.mit.edu/asmlc/ASM.html}) maintained by MIT.

\subsection{The \textit{Neil Gehrels Swift Observatory}}

\par The \textit{Neil Gehrels Swift Observatory}\indexswift, formerly and more commonly known as \textit{Swift}, is a NASA-operated satellite launched from Cape Canaveral on November 20, 2004 \citep{Gehrels_Swift}.  \textit{Swift} was specifically designed to study Gamma Ray Bursts\index{Gamma ray burst} (GRBs), and is notable for its fast slew speed.
\par \textit{Swift} carries three instruments: the X-Ray Telescope (XRT\indexxrt, \citealp{Burrows_XRT}), the wide field-of-view hard X-ray Burst Alert Telescope (BAT\indexbat, \citealp{Krimm_BAT}) and an UltraViolet/Optical Telescope (UVOT\indexuvot, \citealp{Roming_UVOT}).  XRT is the primary instrument on \textit{Swift}: it is a focusing telescope with an effective energy range of 0.2--10\,keV.  Unlike PCA, XRT has imaging capabilities, with a field of view with a radius of 23.6' and an angular resolution of 18''.  The telescope has a minimum time resolution of 1.8\,ms and a minimum energy resolution of $\sim5$\% at 6\,keV.  XRT is operated in one of a number of `operating modes' during each observation, depending on the requirements of the observer.  The two main observing modes are:
\begin{enumerate}
\item Proportional Counting (PC) Mode: a full 2-dimensional image every 2.5\,s.
\item Windowed Timing (WT) Mode: a 1-dimensional image every 2.8\,ms.
\end{enumerate}
Both PC and WT modes also contain full energy information.
\par The main purpose of the wide area BAT\indexbat\ telescope is to identify gamma ray bursts\index{Gamma ray burst} as soon as possible after their onset, so that \textit{Swift} can then slew to them for follow-up observation with XRT.  Due to its large field of view (1.4\,sr) and effective energy range of 15--150\,keV, BAT also provides us with long-term hard X-ray lightcurves of many bright sources in the X-ray sky.  It has a detecting area of 5200\,cm$^2$ and, when operating in survey mode, a time resolution of 5 minutes.
\par The final instrument, UVOT\indexuvot, is intended to take simultaneous optical and ultraviolet observations of sources observed with XRT\indexxrt.  It observes in the wavelength range between 170--650\,nm.

\subsubsection{Data Extraction}

\par XRT\indexxrt\ and BAT\indexbat\ data on non-GRB\index{Gamma ray burst} transients\index{Transient source} are available via online portals maintained by the University of Leicester\footnote{\url{http://www.swift.ac.uk/user_objects/}} and the Goddard Space Flight Centre\footnote{\url{https://swift.gsfc.nasa.gov/results/transients/}} respectively.  The University of Leicester portal automatically extracts lightcurves\index{Lightcurve}, energy spectra\index{Spectroscopy}, images and source positions from raw XRT data of a given target, using the \texttt{xrtpipeline} provided in \texttt{FTOOLS}\index{FTOOLS@\texttt{FTOOLS}}.  The Goddard Space Flight Centre provides ready-made 15--50\,keV lightcurves of 1023\footnote{Count as of October 2018.} X-ray transients, with cadences of either 1 per day or 1 per \textit{Swift} orbit.

\subsection{The \textit{X-Ray Multi-Mirror Mission}}

\par The \textit{X-Ray-Multi Mirror Mission}\indexxmm\ (\textit{XMM-Newton}, \citealp{Jansen_XMM}) is an ESA-operated satellite which was launched from Kourou, French Guiana on December 10, 1999, and is still operating almost 20 years later.  Like \textit{RXTE} and \textit{Swift}, \textit{XMM-Newton} also carries a number of separate instruments: namely the European Photon Imaging Camera (EPIC\indexepic, \citealp{Bignami_EPIC}), the Reflection Grating Spectrometer (RGS\indexrgs, \citealp{denHerder_RGS}) and an Optical Monitor (OM\indexom, \citealp{Mason_OM}).  In the research presented in this thesis, I only make use of data from EPIC.
\par EPIC\indexepic\ consists of three CCD cameras which work independently: two metal-oxide semiconductor CCD cameras (EPIC-MOS1 and EPIC-MOS2) and a single pn CCD camera at the focus of the telescope (EPIC-pn).  All cameras observe in the energy range 0.15--15\,keV, with a Field of View of 30', an angular resolution of 6'' and a maximum energy resolution of $\sim5$\%.  The detectors can be operated in full frame, partial window or timing mode, each of which has a greater time resolution but narrower field of view than the last.  The maximum time resolution achievable by EPIC is 7\,$\mu$s which EPIC-pn is operated in burst mode; a special pn-only variant of timing mode.

\subsubsection{Data Extraction \& Processing}

\par \textit{XMM-Newton}\indexxmm\ data are extracted and processed using the \texttt{SAS}\index{SAS@\texttt{SAS}} software \citep{Ibarra_sas} provided by ESA\footnote{\url{https://www.cosmos.esa.int/web/xmm-newton/sas}}.  These make use of the continuously updated Current Calibration Files (CCF), also provided by ESA.
\par The process of extracting basic data products from the EPIC instruments can be reduced to a number of steps:
\begin{itemize}
\item Use the \texttt{SAS} command \texttt{cifbuild} to create a Calibration Index File (CIF), containing pointers to the information in the CCF needed to reduce the chosen dataset.
\item Use the \texttt{SAS} command \texttt{odfingest} to create a summary file, containing data corrected by the CCF and by the EPIC housekeeping files.
\item Construct a photon event list from EPIC-MOS1 and EPIC-MOS2 using the \texttt{SAS} command \texttt{emproc}, or from EPIC-pn using the command \texttt{epproc}.
\end{itemize}
The event lists\index{Event-mode data} that result from this process can then be filtered using \texttt{evselect}, which allows the user to sort photons by arrival time, spatial co-ordinate and energy channel, among other parameters.  These filtered event lists can then be used to create science data products, such as lightcurves\index{Lightcurve} and energy spectra\index{Spectroscopy}.

\subsection{\textit{Chandra}}

\par The \textit{Chandra X-Ray Observatory}\indexchandra\ (\textit{Chandra}, \citealp{Weisskopf_Chandra}) is a NASA-operated satellite which was launched from Cape Canaveral on July 23, 1999 aboard Space Shuttle \textit{Columbia}.  The mission is considered to be one of NASA's `Great Observatories', along with the \textit{Hubble Space Telescope} (\textit{HST}, e.g. \citealp{Holtzman_Hubble})\index{HST@\textit{HST}}, the \textit{Compton Gamma Ray Observatory} (\textit{CGRO}\index{CGRO@\textit{CGRO}}, \citealp{Gehrels_CGRO}) and the \textit{Spitzer Space Telescope} (\textit{Spitzer}\index{Spitzer@\textit{Spitzer}}, \citealp{Fanson_Spitzer}), which collectively observed the sky between infrared and gamma-ray wavelengths.  \textit{Chandra} was designed to study the X-ray sky between $\sim$0.1--10\,keV, and contains two instruments: the High Resolution Camera (HRC\indexhrc, \citealp{Kenter_HRCI}) and the Advanced CCD Imaging Spectrometer (ACIS\indexacis, \citealp{Nousek_ACIS}).  The spacecraft also carries High and Low Energy Transmission Gratings (HETG and LETG respectively, \citealp{Markert_HETG,Brinkman_LETG}), which can be used in conjunction with the aforementioned detectors to produce high-resolution energy spectra.
\par HRC\indexhrc\ contains two detectors: the HRC Imager (HRC-I) and the HRC Spectrogram (HRC-S).  HRC-I has the largest field of view of any instrument aboard Chandra (30$\times$30'), but no time resolution and only poor spectral resolution.  HRC-S is a long-thin detector strip which is intended to be used as the readout for the LETG.  This detector can also be used in Continuous Clocking mode, in which it has no energy resolution but a timing resolution of 16\,$\mu$s.
\par ACIS\indexacis\ is intended for use either as an imaging camera or as a detector for the output of the HETG.  It has a primary field of view of $16.9\times16.9$', and operates at a maximum time resolution of 2.85\,ms.

\subsubsection{Data Extraction \& Processing}
\par Like \textit{XMM-Newton}, \textit{Chandra}\indexchandra\ data are analysed using a purpose-built suite of tools.  The software for \textit{Chandra} analysis is named \texttt{CIAO}\index{CIAO@\texttt{CIAO}} \citep{Fruscione_Ciao}, and is freely provided by Harvard University\footnote{\url{http://cxc.harvard.edu/ciao/}}.  \texttt{Ciao} filters and bins data based on any of the four possible parameters stored for a photon event (time, energy and two spatial co-ordinates), and facilitates the production of lightcurves\index{Lightcurve}, images and energy spectra\index{Spectroscopy}.

\subsection{\textit{Suzaku}}

\par \textit{Suzaku}\indexsuzaku\ \citep{Mitsuda_Suzaku} was a JAXA-operated satellite which operated from its launch from the Uchinoura Space Center, Japan on July 10, 2005 until being decomissioned on September 2, 2015.  The mission was intended for X-ray spectroscopy; however the satellite's primary instrument, the X-Ray Spectrometer (XRS\indexxrs, \citealp{Kelley_XRS}), lost all of its liquid helium coolant within the first month of operation, rendering it effectively unusable.  The remaining instruments aboard \textit{Suzaku}, namely the X-Ray Imaging Spectrometers (XIS\indexxis, \citealp{Koyama_XIS}) and the Hard X-Ray Detector (HXD\indexhxd, \citealp{Takahashi_HXD}) were unaffected by the malfunction and continued to operate normally throughout the spacecraft's lifetime.
\par XIS\indexxis\ consists of four X-ray cameras, with a total field of view of $17.8\times17.8$' and a spatial resolution of $\geq1.6$''.  One of these cameras (XIS2) was damaged by a micrometeorite, and was switched off on November 9, 2006.  The instrument has a good spectral resolution over its operational energy range of 0.2--10\,keV, peaking at $\sim170$\,eV at the upper end of this range.  Standard XIS observation modes provide a time resolution of 8\,s, corresponding to the duration of a single CCD exposure.  This timing resolution can be improved by a factor of a few by sacrificing imaging information in other observation modes, such as the one-dimensional P-sum mode with a timing resolution of 7.8\,ms.
\par HXD\indexhxd\ complements XIS\indexxis\ at higher energies, with an effective energy range of 12--600\,keV.  The instrument has an energy resolution of $\sim3$\,keV below 60\,keV, and $\sim7$--$8\sqrt{E}$\% above 60\,keV, where $E$ is the energy in MeV.  The instrument has an optimum time resolution of 61\,$\mu$s.
\par As with most instruments, there exist standard procedures when reducing and analysing data from \textit{Suzaku}\indexsuzaku.  First of all, the data must be reprocessed using the \texttt{aepipeline} script available as a part of \texttt{FTOOLS}\index{FTOOLS@\texttt{FTOOLS}}.  Lightcurves, images and spectra can then be extracted using the standard multimission tools also available in FTOOLS\index{FTOOLS@\texttt{FTOOLS}}.  Note that, for XIS\indexxis, backgrounds for each of the four detectors should be extracted separately due to differential degradation over the lifetime of the mission.

\subsection{The \textit{Nuclear Spectroscopic Telescope Array}}

\par The \textit{Nuclear Spectroscopic Telescope Array}\indexnustar\ (\textit{NuSTAR}, \citealp{Harrison_NuStar}) is a NASA-operated satellite launched from the \textit{Stargazer} aircraft off the coast of the Marshall Islands on June 13, 2012.  The satellite carries two co-pointing X-ray telescopes, which are matched with Focal Plane Modules referred to as FPMA and FPMB.  These detectors are sensitive and calibrated in the range 3--78\,keV, and each has an effective area of $\sim450$\,cm$^2$ at $\sim10$\,keV.  The telescopes have a field of view of $12.2\times12.2$', and a full-width half-maximum angular resolution of $\gtrsim18$''.  Events are detected by \textit{NuSTAR} with a time resolution of 2\,$\mu$s, while the energy resolution at 50\,keV is around 0.4\,keV.
\par \textit{NuSTAR} is the first instrument able to focus hard X-rays ($\gtrsim10$\,keV) to produce relatively clear images.  Additionally \textit{NuSTAR} does not suffer from issues due to pile-up\index{Pile-up} (an instrumental effect in most instruments which overestimates the hard flux from bright objects, see Section \ref{sec:pude}), but it does suffer from significant dead-time\index{Dead-time} (which underestimates flux from bright objects, see also Section \ref{sec:pude}).  See also \citet{Bachetti_dt}.
\par For the research presented in this thesis, I reduce data from \textit{NuSTAR} using the \texttt{nupipeline} script from the freely available \textit{NuSTAR} Data Analysis Software (NuSTARDAS\index{NuSTARDAS@\texttt{NuSTARDAS}}\footnote{\url{https://heasarc.gsfc.nasa.gov/docs/nustar/analysis/nustar_swguide.pdf}}).  This script automatically runs all of the relevant tasks required to reduce \textit{NuSTAR} data, including flagging of events, flagging of bad pixels and correcting for detector gain.  It also calculates sky co-ordinates and energy for each event.

\subsection{The \textit{International Gamma-Ray Astrophysics Laboratory}}

\par The \textit{INTernational Gamma-Ray Astrophysics Laboratory} (\textit{INTEGRAL}\indexintegral, \citealp{Winkler_INTEGRAL}) is an ESA-operated satellite which was launched from Kazakhstan's Baikonur Cosmodrome on October 17, 2002.  The primary purpose of the mission is the spectroscopy of astrophysical sources in the hard X-ray and soft gamma-ray bands, between $\sim4$--10,000\,keV.
\par \textit{INTEGRAL} carries two main scientific instruments: the SPectrometer on \textit{INTEGRAL} (SPI\indexspi, \citealp{Vedrenne_SPI}) and the Imager on-Board \textit{INTEGRAL} (IBIS\indexibis, \citealp{Winkler_IBIS}).  The spacecraft also carries an X-ray monitor (JEM-X\indexjemx, \citealp{Schnopper_JEMX}) and an optical camera (OMC\indexomc, \citealp{Gimenez_OMC}).  SPI\indexspi\ is a high-resolution gamma-ray spectrometer, with an energy resolution of $\sim$2.2\,keV at 1.33\,MeV and an energy range of 18\,keV to 8\,MeV.  It has a field of view of $>14^\circ$, an angular resolution of 2.5$^\circ$, a time resolution of 0.129\,ms and a collecting area of $\sim500$\,cm$^2$.
\par IBIS\indexibis\ has a wider energy range than SPI\indexspi, from 15\,keV to 10\,MeV, and a larger collecting area at 2600\,cm$^2$ at 100\,keV.  The timing accuracy is 61\,$\mu$s, but the energy resolution peaks at only 8\% at $\sim100$\,keV.  As IBIS is an imager, it has a good angular resolution of $\sim12$' and a fully-coded field of view of $8\times8$''.  IBIS contains two detector planes stacked on top of each other; the top layer (ISGRI, \citealp{Lebrun_ISGRI}) is designed to detect low-energy gamma rays, while the lower layer (PICsIT, \citealp{Labanti_Picsit}) is designed to detect the higher-energy gamma rays which pass through ISGRI undetected.
\par Data products from all four instruments are available via the \textit{INTEGRAL} Heavens portal \citep{Lubinski_Heavens} maintained by the \textit{INTEGRAL} Science Data Centre \footnote{\url{https://www.isdc.unige.ch/heavens/}}.  This portal provides images, lightcurves\index{Lightcurve} and spectra\index{Spectroscopy} of data taken from archived \textit{INTEGRAL} observations.

\subsection{Dead-time and Pile-up}

\label{sec:pude}

\par All X-ray telescopes suffer from a number of instrumental biases, caused by a number of instrumental effects.  Two of the most significant of these are dead-time\index{Dead-time} and pile-up\index{Pile-up}, which are both caused by the limitations of CCD detectors.  When a photon is detected, a CCD takes a finite time to respond to it to form a digital signal.  During this response time, known as the `dead-time', the instrument is unable to respond to any additional photons.  This means that the instrument is `blind' to photons for a period of time after each registered event.  The dead-time of a given instrument is generally of the order of a few $\mu$s per event.  For high incident count rates, dead-time can lead to a significantly reduced reported count rate.  In addition to this there is an effect on the statistics of photon arrival times.  Photon arrival times from an astrophysical source are generally Poisson distributed\index{Poisson distribution}; however, the existence of dead-time means that two consecutive reported photon arrival times are no longer independent of each other.  This in turn can effect the level and the shape of the noise component seen when analysing Fourier spectra\index{Fourier analysis} of the data (see Section \ref{sec:fou}).
\par Similarly, pile-up\index{Pile-up} is an effect which is mostly seen in data from bright sources.  Pile-up occurs when two photons coincide both temporally and spatially in such a way that the detector interprets them as a single event.  This causes two photon events to be recorded as a single photon event with an energy equal to the sum of the two.  This effect causes the hard emission from a source to be over-reported, and the soft emission to be under-reported.  The exact magnitude of the effects from dead-time to pile-up varies from detector to detector; for most detectors the effects are well understood and can therefore be estimated and corrected for.

\section{Methods \& Techniques}

\label{sec:tec}

\par To extract meaningful physics from the data provided by the space-based observatories described in Section \ref{sec:sat}, I use a number of mathematical and analytical techniques.  In a nutshell, I analyse three main properties of the data:
\begin{enumerate}
\item \textbf{Lightcurve Morphology:}\index{Lightcurve} Describing how the intensity of an object in a given energy band varies as a function of time.
\item \textbf{Timing Analysis:} Using Fourier\index{Fourier analysis} and Lomb-Scargle\index{Lomb-Scargle periodogram} analyses to identify periodic and quasi-periodic\index{Quasi-periodic oscillation} variability\index{Variability} in the data, and how these change with energy and time.
\item \textbf{Energy Spectral Analysis\index{Spectroscopy}:} Measuring the shape of the energy spectrum of an object, particularly by using hardness ratios (see e.g. Section \ref{sec:hids}), and analysing how this changes over time.
\end{enumerate}
\par Some of the techniques I used to explore these properties are detailed in this section.

\subsection{Lightcurve Morphology}

\label{sec:LCMorph}

\par The morphology of a lightcurve\index{Lightcurve}, how the brightness of a source varies over time, can tell us about the physical processes at work in a system.  For example, the rise and fall-times of an X-ray burst\index{X-ray burst} can be matched with characteristic physical timescales of an accreting system to better understand which of them play roles in generating the bursts.  However, quantifying these shapes over short timescales, or in low-quality datasets, can be difficult.  As such, methods exist to help analyse the morphology of these difficult datasets, and I employ a number of them in the research presented in this thesis.

\subsubsection{Lightcurve Folding}

\label{sec:normfold}

\par In systems with periodic or quasi-periodic behaviour\index{Quasi-periodic oscillation}, it is important to understand the morphology of a single cycle of the behaviour.  In order to improve the statistics on such data, one can take the average of many cycles, resulting in a single averaged cycle with a greatly increased signal-to-noise ratio.  The process of obtaining this average cycle is known as `folding'\index{Folding} data.
\par To fold a periodic dataset with a known period $p$, the time $t$ associated with each datapoint must be converted into a phase $\phi$ (for $0\leq\phi<1$) such that datapoints at the same stage of different oscillations have the same $\phi$.  This can be done using the formula:
\begin{equation}
\phi(t)=\Phi(t)\mod1=(t-t_0)/p\mod1=(t-t_0)/p-N_t
\label{eq:simfold}
\end{equation}
Where $\Phi(t)$ is the fractional number of cycles which have elapsed between times $t_0$ and $t$ for an arbitrary start time $t_0$, and $N_t$ is the integer number of complete cycles which have occured between times $t_0$ and $t$.
\par This procedure can also be thought of as cutting a lightcurve into a number of segments each of length $p$.  Each datapoint in each segment can then be assigned a $\phi$ value, where $\phi$ corresponds to the datapoint's fractional position within its segment.  Once this information has been found for all datapoints, the data can be rebinned in $\phi$-space to `stack` every cycle on top of one another.  I illustrate this process visually in Figure \ref{fig:Folding}.  This method is useful for finding mean oscillation profiles when $p$ is very close to a constant, such as finding the mean pulse profile of a pulsar over a small number of rotations.  However in many cases, such as in quasi-periodic oscillations\index{Quasi-periodic oscillation} (QPOs), $p$ is not a constant.  More complex methods must then be used to find the mean pulse profile.

\begin{figure}
    \includegraphics[width=\columnwidth, trim = 0mm 220mm 0mm 0mm,clip]{images/folding.eps}
    \includegraphics[width=\columnwidth, trim = 0mm 155mm 0mm 65mm,clip]{images/folding.eps}
    \includegraphics[width=\columnwidth, trim = 0mm 20mm 0mm 135mm,clip]{images/folding.eps}
    \captionsetup{singlelinecheck=off}
    \caption[A cartoon illustrating the process of folding a periodic lightcurve with a known period.]{A cartoon illustrating the process of folding\index{Folding} a periodic lightcurve with a known period, which I describe mathematically in Section \ref{sec:normfold}.  \textbf{1:} a simulated lightcurve with errors.  \textbf{2:} Divide the lightcurve into sections by cutting it at every time coordinate $Np$, where $p$ is the known period and $N$ is any integer.  Each data point may now be given a phase coordinate $\phi$ in addition to its time coordinate $t$, where $\phi=(t/p)-N$ for $N$ such that $0\leq\phi<1$.  \textbf{3:} The lightcurve segments can be realigned in phase-space, such that points with the same value of $\phi$ sit at the same $x$-coordinate.  \textbf{4:} All points within given bins in $\phi$-space are averaged to create a lightcurve corresponding to the averaged oscillations of the original lightcurve.  The folding has revealed a peak at $\phi=0.75$ which was not apparent in the unfolded data.}
   \label{fig:Folding}
\end{figure}

\subsubsection{Flare-Finding Algorithm}
\label{sec:Flares}

\par To fold a quasi-periodic oscillation\index{Quasi-periodic oscillation}, such as the `heartbeat'\indexrho\ flares\index{Flare} seen in GRS 1915+105\index{GRS 1915+105} and IGR J17091-3624\index{IGR J17091-3624}, it is first important to find the time-coordinates which characterise the beginning, end and peak of each flare.  To this end, I have created an algorithm to locate individual flares in a dataset containing non-periodic high-amplitude flares. The algorithm is performed as such:

\begin{enumerate}
  \item Choose some threshold values $T_L$ and $T_H$.  Set the y-value of all datapoints with $y<T_L$ to zero.
  \item Retrieve the time co-ordinate of the highest value remaining in the dataset.  Call this value $t_m$ and store it in a list.
  \item Set the $y$-value of the point at $t_m$ to zero.
  \item Scan forwards from $t_m$ by selecting the datapoint at $t_m+\Delta t$, where $\Delta t$ is the time resolution of the data.  If the selected point has a nonzero value, set it to zero and move to the next point.  If the selected point has a zero value, move to step 5.
  \item Scan backwards from $t_m$ by selecting the datapoint at $t_m-\Delta t$.  If the selected point has a nonzero value, set it to zero and move to the previous point.  If the selected point has a zero value, move to step 6.
  \item Retrieve the y-co-ordinate of the highest value remaining in the dataset.  Call this $y_m$.
  \item If $y_m>T_H$, repeat steps 2--7.  If $y_m<T_H$, proceed to step 8.
  \item Restore the original dataset.
  \item Retrieve the list of $t_m$ values found in step (ii).  Sort them in order of size.
  \item For each pair of adjacent $t_m$ values, find the $t$-coordinate of the datapoint between them with the lowest y-value.  Call these values $t_c$.
  \item This list of $t_c$ can now be used to demarcate the border between peaks.
\end{enumerate}

The process can be thought of as using $T_L$ to divide the data into a number of discrete segments of non-zero data, and treating the peak of each segment as the peak of a flare.  I illustrate this process visually in Figure \ref{fig:BurstAlg}.

\begin{figure}
    \includegraphics[width=\columnwidth, trim = 0mm 30mm 0mm 28mm]{images/steps.eps}
    \captionsetup{singlelinecheck=off}
    \caption[A cartoon illustrating the procedure of the algorithm described in Section \ref{sec:Flares}.]{A cartoon illustrating the procedure of my algorithm which I describe in section \ref{sec:Flares}.  From top-left: (i) An untouched lightcurve\index{Lightcurve}.  (ii) The lightcurve with all $y<T_L$ removed.  (iii) The lightcurve with all contiguous nonzero regions with $\max(y)<T_H$ removed.  (iv) The $t$-coordinates of peak $y$-values $t_m$.  (v) The restored lightcurve with the $t_m$ highlighted.  (vi) The boundaries between adjacent peaks.}
   \label{fig:BurstAlg}
\end{figure}

\par The threshold values $T_L$ and $T_H$ can also be procedurally generated for a given dataset:

\begin{enumerate}
  \item Select a small section of the dataset or a similar dataset (containing $\sim20$ peaks by eye) and note the time-coordinates $t_e$ of all peaks found by eye.
  \item Let $P_L$ and $P_H$ be two arbitrary values in the range $[0,100]$.
  \item Let $T_L$ ($T_H$) be the $P_L$th ($P_H$th) percentile of the y-values of the subsection of dataset.
  \item Run the flare-finding algorithm up to step 9.  Save the list of $t_m$.
  \item Split the dataset into bins on the x-axis such as the bin width $b\ll p$, where $p$ is the rough x-axis separation between peaks.
  \item For each bin, note if you found any value in $t_m$ falls in the bin and note if any value of $t_e$ falls in the bin.
  \item Using each bin as a trial, compute the Heidke Skill Score\index{Heidke skill score} \citep{Heidke_SKSC} of the algorithm with the method of finding peaks by eye:
  \begin{equation}HSS = \frac{2(AD-BC)}{(A+B)(B+D)+(A+C)(C+D)}
  \label{eq:HSS}
  \end{equation}
  Where $A$ is the number of bins that contain both $t_e$ and $t_m$, $B$ ($C$) is the number of bins that contain only $t_m$ ($t_e$) and $D$ is the number of bins which contain neither \citep{Kok_YesNo}.
  \item Repeat steps (iii)--(vii) for all values of $P_H>P_L$ for $P_L$ and $P_H$ in $[1,100]$.  Use a sensible value for the resolution of $P_L$ and $P_H$.  Save the HSS for each pair of values
  \item Locate the maximum value of HSS, and note the $P_L$ and $P_H$ values used to generate it.  Use these values to generate final $T_L$ and $T_H$ values.
\end{enumerate}

%I show an example of Heidke skill score grid for this algorithm, applied to a Class IV observation, in Figure \ref{fig:Heidke}.

%\begin{figure}
%    \includegraphics[width=\columnwidth, trim = 0mm 10mm 0mm 10mm]{images/HSS_J.eps}
%    \captionsetup{singlelinecheck=off}
%   \caption[The Heidke Skill score of a Class IV observation of IGR J17091-3624 for a selection of different values $P_L$ and $P_H$ (low and high threshold respectively).]{The Heidke Skill score of a Class IV observation of IGR J17091-3624 for a selection of different values $P_L$ and $P_H$ (low and high threshold respectively).}
%   \label{fig:Heidke}
%\end{figure}

\subsubsection{Variable Period Lightcurve Folding}

\label{sec:vfold}

\par With the values $t_m$ and $t_c$ found using the algorithm described above, it is possible to recast Equation \ref{eq:simfold} to fold\index{Folding} data over a high-amplitude but quasi-periodic oscillation\index{Quasi-periodic oscillation}.  I detail my method below:

\begin{enumerate}
  \item Take the ascending list of peak $t$-coordinates $t_m$.  Assign the first element a value $\Phi=0$.
  \item Assign each other point in $t_m$ an integer value $\Phi(t)$, such that the $\Phi$ value of the $i$th value of $t_m$ is defined as:
  \begin{equation}
  \Phi(t_m^{i})=\Phi(t_m^{i-1})+1, i\geq2
  \end{equation}
  \item If the troughs between peaks are well-defined, proceed to step 4.  Otherwise, skip to step 6.
  \item If the $t$-coordinate of the first datapoint in $t_c$ is less than the $t$-coordinate of the first datapoint in $t_m$, assign $\Phi(t_c^1)=-0.5$.  Otherwise, assign $\Phi(t_c^1)=0.5$.
  \item Assign each other point in $t_m$ a value $\Phi(x)$, such that the $\Phi$ value of the $i$th value of $t_c$ is defined as:
  \begin{equation}
  \Phi(t_c^{i})=\Phi(t_c^{i-1})+1, i\geq2
  \end{equation}
  \item Create a general function defining $\Phi$ for all $t$ by fitting the $t$ and $\Phi$ values of $t_m$ (and $t_c$, if used) with a monotonically increasing univariate cubic spline\index{Spline}\footnote{Computationally realised as \texttt{PchipInterpolator} in the \texttt{scipy} package for Python \citep{NumPy}.} $S(t)$.
  \item Define the phase $\phi(t)$ of an arbitrary time $t$ as $\phi(t)=S(t)\mod1$.
\end{enumerate}

\par With a phase defined for all points in time, the data can be manipulated as if it had been folded\index{Folding} in the usual way.  If the trough times in addition to the peak times are used to construct the spline, then the folded data are more accurate: however, by definition the rising part of each flare will occupy phases 0.5--1.0, while the falling part will occupy 0.0--0.5, so any asymmetry in the rise and fall times of the average flare is lost.
\par This method assumes that $d\phi/dt$ is continuous at all $t$, but this assumption is not necessarily true for cases in which each flare is a discrete event.  Consider for example the path of a juggling ball.  During each throw, the ball takes some time $\tau$ to complete its arc, moving from $\phi=0$ to $\phi=1$.  However, the value of $\tau$, and hence the value of $d\phi/dt$, depends on the impulse given to the ball at the moment of being thrown.  As such, $d\phi/dt$ is discontinuous at the point of the ball being thrown, and my method outlined here would not correctly fold a curve of its height as a function of time.

\subsection{Timing Analysis}

\label{sec:fou}

\par Another way of looking at the variability of an astrophysical source is by looking in the frequency domain.  Well-established mathematical techniques, in particular Fourier spectroscopy\index{Fourier analysis}, are able to deconvolve a time series into series of sine waves.  The amplitudes of these sine waves indicate how much variability in the system takes place at a given frequency.

\subsubsection{Fourier Analysis}

\par Fourier analysis\index{Fourier analysis} \citep{Fourier} is the most common way to perform frequency analysis on a time series.  The Fourier transform $\hat{f}(\nu)$ of a time series $f(t)$ is defined as:
\begin{equation}
\hat{f}(\nu)=\int_\infty^\infty f(t)e^{-2\pi it\nu} dt
\end{equation}
Where $\nu$ is the frequency to be probed and $i\equiv\sqrt{-1}$.  The magnitudes of the complex values $\hat{f}(\nu)$ describe the amplitude of the sine wave deconvolution at frequency $\nu$, while the arguments describe the relative phase of each of these sine waves.  As such, a plot of $|\hat{f}(\nu)|$ against $\nu$, known as a Fourier spectrum, can highlight the frequencies at which the time series shows oscillations.  A strictly periodic oscillation shows up in a Fourier spectrum as a delta spike at a single frequency $\nu_p$; if the oscillation is not strictly sinusoidal, then there may also be spikes present at the harmonic\index{Harmonic} frequencies $N\nu_p$ for any $N\in\mathbb{N}$.
\par An oscillation which is not strictly periodic is known as a quasi-periodic oscillation\index{Quasi-periodic oscillation}, or QPO.  The non-periodic component in a QPO can be related to its frequency (such as a spinning object which slows down over time), its amplitude (such as a damped harmonic oscillator) or some internal phase drift (such as the X-ray flux from an accreting X-ray pulsar\index{Pulsar} on which the hotspot is migrating, see e.g. \citealp{Patruno_Phase}).  A quasi-periodic oscillation shows up in a Fourier spectrum as a Lorentzian, defined by its amplitude and its quality factor $q$\indexq.  Quality factor is in turn defined as peak frequency divided by full-width half-maximum\footnote{The full-width half-maximum, or FWHM, of a QPO or spectral line is a measure of the width of the feature.  First calculate the amplitude $A$ of the feature above the local continuum level $k$.  The width of the feature in the $x$-direction at $y=k+\frac{A}{2}$ is its FWHM.}; for a QPO with a wandering frequency, this represents approximately the number of oscillations over which the QPO remains coherent.
\label{sec:convolver}
\par Fourier analysis\index{Fourier analysis} was envisioned to analyse continuous, infinite data.  However, physical data differs from this ideal case in two important ways:
\begin{enumerate}
\item Physical data are discrete rather than continuous, consisting of samples taken at a finite rate $\sigma$.
\item Physical data are finite rather than infinite, being taken in some window of length $w$.
\end{enumerate}
As such, as I show in Figure \ref{fig:convolve}, physical data consists of a time series convolved with both a windowing function and a sampling function.  Each of these convolutions adds spurious features to the power spectrum produced from the data.

\begin{figure}
    \includegraphics[width=\columnwidth, trim = 0mm 10mm 0mm 10mm]{images/convolve.eps}
    \captionsetup{singlelinecheck=off}
    \caption[A representation of how a continuous variable is convolved with a windowing function and a sampling function to yield physical data.]{A representation of how a continuous variable $y(t)$ is convolved with a windowing function $w(t)$ and a sampling function $s(t)$ to yield physical data.  I describe the effects of these convolutions in Section \ref{sec:convolver}.  The bottom panel shows how aliasing\index{Aliasing} arises, showing that sine waves of two different frequencies can be fit to the data: one with a frequency $\nu$ equal to that in the original dataset, and one of frequency $\sigma-\nu$ where $\sigma$ is the sampling frequency.  This explains the presence of aliased peaks in discrete data.}
   \label{fig:convolve}
\end{figure}

\par The convolution with a sampling function adds so-called `aliased'\index{Aliasing} peaks to the power spectrum of a given dataset.  For each peak in the power spectrum at frequency $\nu$, there will also be a peak present at a frequency of $\sigma-\nu$, where $\sigma$ is the sampling frequency.  This peak can be understood as the beat frequency between the oscillation in the data and the sampling frequency (see also the lower panel of Figure \ref{fig:convolve} for a visual explanation), and contains no additional information on the system.  To avoid these aliased peaks, values of $\hat{f}(\nu)$ outside of the range $0<\nu\leq\sigma/2$ are discarded.  The frequency $\sigma/2$, the maximum frequency at which one can extract useful information on a parameter sampled at constant frequency $\sigma$, is known as the Nyquist frequency\index{Nyquist frequency}.
\par In general, the convolution with a windowing function causes peaks in the power spectrum to be broadened; an effect known as `spectral leakage'\index{Spectral leakage}.  The form of this broadening depends on the windowing function which is being used.  Generally, physical data has been convolved with a so-called `boxcar' window; i.e., a function which takes a value of 1 during the period of measurement and 0 elsewhere.  A convolution with a boxcar window causes each peak in the power spectrum to be accompanied by a number of lower-amplitude sidelobes either side of it in frequency space; this serves to smear out a power spectrum and causes some information to be lost.  Other windows can be applied to data to attempt to lessen this effect; for example, convolving a dataset with a triangular or Gaussian window instead of a boxcar.  Many non-boxcar windows have been formulated to lessen the effect of spectral leakage, but it is impossible to remove the effect completely when working with a finite dataset.

\subsubsection{Fast Fourier Transform}

\par Taking the Fourier transform\index{Fourier analysis} of a time series is a computationally expensive procedure.  As such, it is common practice to instead use Fast Fourier Transform (FFT)\index{Fast Fourier transform} algorithms; computationally fast algorithms which specialise in finding the Fourier transform of evenly-spaced series.
\par One such FFT algorithm\index{Fast Fourier transform} is the Cooley-Tukey\index{Cooley-Tukey algorithm}\footnote{Computationaly realised as \texttt{fft} in the \texttt{scipy.fftpack} package for Python \citep{NumPy}.} algorithm \citep{Cooley_FFT}.  The Cooley-Tukey algorithm speeds up the Fourier transform process by recursively dividing a dataset in half to make many segments.  It uses the fact that the discrete Fourier transform of a single point is equal to itself, and then reconstructs the complete Fourier spectrum from these results. Unlike the basic Fourier transform, the Cooley-Tukey algorithm is only able to transform series which are evenly spaced in time and consisting of $2^N$ datapoints, for $N\in\mathbb{N}$.
\par The amplitude error of a Fast Fourier Transform\index{Fast Fourier transform} of a noise process is 100\%.  There are two ways to reduce this error to a level at which the data can be meaningfully analysed:
\begin{enumerate}
\item The original time series can be split into a number of equal-length windows.  The Fast Fourier-Transforms of these windows can be calculated independently of each other, and then averaged to create the mean FFT of the dataset.
\item The resultant power spectrum can be rebinned in frequency space.
\end{enumerate}
\par Propagating errors in the usual way, this results in a final error on Fourier power amplitude $\delta|\hat{f}(\nu)|^2$ of:
\begin{equation}
\delta|\hat{f}(\nu)|^2=\frac{|\hat{f}(\nu)|^2}{\sqrt{MW}}
\end{equation}
Where $W$ is the number of windows the original dataset was divided into, and $M$ is the number of frequency bins which were averaged to obtain the Fourier power at frequency $\nu$.  Increasing $W$ increases the minimum frequency at which the Fourier power of the dataset can be probed, while increasing $M$ decreases the resolution of the spectrum in frequency space.

\subsubsection{Normalising the Fourier Transform}

\par To understand the statistical significance of features in a power spectrum\index{Fourier analysis}, it is important to normalise the results in a standard and well-understood way.  One such method of normalisation is the `Leahy' normalisation\index{Leahy normalisation} \citep{Leahy_Norm}, defined as:
\begin{equation}
L(\nu)=\frac{2\times|\hat{f}(\nu)|^2}{n_{p}}
\end{equation}
Where $n_p$ is the total number of photon counts in the original dataset.  This normalisation has the property that pure Poisson\index{Poisson distribution} noise has a Leahy-normalised power of 2\footnote{In practise, due to instrumental dead-time effects meaning photon arrivals are not strictly independent, Poisson noise in astrophysical data tends to yield a Leahy-normalised power of slightly less than 2}.
\par I use one additional power spectrum normalisation in the work presented in this thesis: the RMS normalisation\index{RMS normalisation}.  This is defined as:
\begin{equation}
R(\nu)=\frac{(L(\nu)-2)r_s}{(r_s-r_b)^2}=\frac{2\left(|\hat{f}(\nu)|^2-Tr_s\right)}{T(r_s-r_b)^2}
\end{equation}
\par Where $T$ is the total time duration of all data used to produce the power spectrum, $r_s$ is the mean source count rate and $r_b$ is the mean background rate.  In this normalisation, uncorrelated Poisson noise corresponds to a power of zero.  Additionally, the power spectrum has the property that the integral of $R(\nu)$ between two frequencies is equal to the squared root-mean squared amplitude (RMS\index{RMS}\index{Root-mean squared variability|see {RMS}}$^2$) of the variability\index{Variability} of the original time series in that frequency band.

\subsubsection{Lomb-Scargle Periodograms}

\par Fast Fourier transforms\index{Fast Fourier transform} are unable to process unevenly spaced time series.  Additionally, while mathematical Fourier transforms\index{Fourier analysis} can in general process unevenly spaced datasets, the effects of aliasing\index{Aliasing} become increasingly complex and difficult to disentangle from real signal.  In these cases, a method known as the Lomb-Scargle periodogram\index{Lomb-Scargle periodogram}, based on proposals by \citet{Lomb_LombScargle} and \citet{Scargle_LombScargle}, can be used.
\par The Lomb-Scargle periodogram\index{Lomb-Scargle periodogram} can be thought of as the result of fitting sinusoids of frequency $\nu$ to a time series, and constructing a spectrum using the $\chi^2$ value of the fit of the sinusoid at each $\nu$.  Unlike a Fourier spectrum\index{Fourier analysis} of unevenly spaced data, the Lomb-Scargle periodogram of unevenly spaced data is statistically well-behaved as long as the noise component of the dataset is uncorrelated.
\par Unfortunately, due to dead-time\index{Dead-time} effects present in all X-ray telescopes, white noise in real datasets is not uncorrelated and so the statistical properties of the Lomb-Scargle spectrogram are generally not well-defined.  In this case, bootstrapping techniques can be used to estimate the significances of features in the power spectrum.

\subsection{Energy Spectral Analysis}

\par Energy spectral\index{Spectroscopy} analysis is another powerful tool available to understand the physical processes at work in astrophysical systems.  The distribution of arriving photons as a function of energy can be fit to physical models which, assuming a given system geometry, can provide estimates of various system parameters.
\par The disadvantage of spectral fitting is the aforementioned assumptions that one has to make.  A number of well-studied spectral models of LMXBs\index{X-ray binary!Low mass} exist, which are able to return estimates for values such as inner disk radius, black hole mass and spin when fit to data.  However, the values that different models return often contradict each other, and thus the values that a study infers for these parameters depend heavily on the system physics and geometry that the modeller assumes.

\subsubsection{Hardness-Intensity Diagrams}

\label{sec:hids}

\par A model-independent way to study the spectral properties of a source is by using colours\index{Colour}, also known as hardness ratios.  To obtain the colour of a source, I define two non-overlapping energy bands $A$ and $B$ with $B>A$.  The hardness ratio is then defined as $H(t)=r_B(t)/r_A(t)$, where $r_X(t)$ is the photon arrival rate in the band $X$.  The hardness ratio gives basic information on the shape of the energy spectrum without assuming a physical model.
\par Hardness ratios are often paired with intensity (the total flux of the object in some energy band which includes A and B) to create `hardness-intensity diagrams'\index{Hardness-intensity diagram} (HIDs) to explore how the source spectrally varies over time.  To explain what the shape of an HID can tell us about the spectral evolution of a source, consider the following examples of HIDs for a black body spectrum with temperature $T(t)$ and normalisation $n(t)$:
\begin{enumerate}
\item $T(t)=1, n(t)=\sin(t)$: in this example, the brightness of the source changes over time but the shape of its spectrum does not change.  As such the hardness is a constant, and the system traces a vertical line in hardness-intensity space (Figure \ref{fig:HIDexp}, Panel 1).
\begin{figure}
    \includegraphics[width=\columnwidth, trim = 0mm 25mm 0mm 25mm]{images/hidexp.eps}
    \captionsetup{singlelinecheck=off}
    \caption[Hardness-Intensity diagrams of black bodies with temperatures and normalisations described by various functional forms.]{Hardness-Intensity diagrams\index{Hardness-intensity diagram} of black bodies with temperatures and normalisations described by various functional forms $T(t)$ and $n(t)$ which I give in Section \ref{sec:hids}.  The plots show how HIDs differ between sources with \textbf{1)} changing brightness but no spectral change, \textbf{2)} changing temperature, \textbf{3)} changing temperature and normalisation in antiphase, and \textbf{4)} changing temperature and normalisation out of phase.  The shape of and direction of a loop in a HID can therefore give us information about the physical processes underlying spectral variability.}
   \label{fig:HIDexp}
\end{figure}
\item $T(t)=\sin(t), n(t)=1$: in this example, the spectrum of the source changes over time, resulting in a curved track in hardness-intensity space (Figure \ref{fig:HIDexp}, Panel 2).
\item $T(t)=\sin(t), n(t)=\sin(t-\pi)$: if two or more spectral parameters are varying at once, the track can become move complex.  If these parameters are varying in phase or antiphase, a single track is traced (Figure \ref{fig:HIDexp}, Panel 3).
\item $T(t)=\sin(t), n(t)=\sin\left(t+\frac{\pi}{3}\right)$: when parameters are varying out of phase with each other, the track of the object in a HID can take the form of a closed loop (Figure \ref{fig:HIDexp}, Panel 4).  I use$\frac{\pi}{3}$ here as an arbitrary phase shift.
\end{enumerate}
Case 4 is interesting, as it indicates the presence of a phase lag between two or more physical components of the system.  The direction in which the loop is executed over time can be used to infer the sign of this lag.  This in turn can give constraints on the causal links between components of a system, in turn giving constraints on physical models proposed to describe them.  The tracing of a loop in a hardness-intensity diagram\index{Hardness-intensity diagram} is known as hysteresis\index{Hysteresis}.

\subsubsection{Phase-Resolved Spectroscopy}

\label{sec:phasresspec}

\par Like lightcurves, HIDs\index{Hardness-intensity diagram} and time-resolved spectra can be difficult to analyse when constructed from data with poor statistics.  If the flux from a source is variable\index{Variability} in a periodic or quasi-periodic way\index{Quasi-periodic oscillation}, a modified version of the folding\index{Folding} algorithms detailed in Section \ref{sec:LCMorph} can be used to analyse the spectral evolution of an average cycle:
\begin{itemize}
\item Obtain the function $\phi(t)$ to describe how phase varies as a function of time.
\item Split the interval $[0,1)$ into a number of sub-intervals $i$.
\item For each sub-interval $i$, compile a list of good time intervals (GTIs) denoting periods of time during which $\phi(t)\in i$.
\item For each list of GTIs, filter the original dataset such that it only contains photons which arrived during one of the intervals.
\item From each new filtered dataset, a spectrum or hardness ratio can be calculated.  This can be compared with the spectra or hardness ratios taken from the other filtered datasets to analyse how the spectrum of the source varies as a function of phase.
\end{itemize}
This technique is known as phase-resolved spectroscopy\index{Spectroscopy!Phase-resolved}.  An example of the use of this method with the algoritm I describe in Section \ref{sec:vfold} is presented in \citet{Wang_Reflect}.



